{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65633d4d",
   "metadata": {},
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1023c2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, Literal\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c99d0c",
   "metadata": {},
   "source": [
    "## Import the keys and define the LLM model and Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8f7d7c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "os.environ['TAVILY_API_KEY'] = TAVILY_API_KEY\n",
    "\n",
    "os.environ['HF_TOKEN']=os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model=\"llama-3.1-8b-instant\", temperature=0)\n",
    "embedding_model=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66cd340",
   "metadata": {},
   "source": [
    "## Define Agent State schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e7790b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State definition\n",
    "class AgentState(TypedDict):\n",
    "    query: str\n",
    "    context: str\n",
    "    route: Literal[\"rag\", \"web_search\", \"llm\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f393e69",
   "metadata": {},
   "source": [
    "## Define the Web search agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "42aa76d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web search setup\n",
    "tavily_search = TavilySearchAPIWrapper()\n",
    "\n",
    "def web_search_agent(state: AgentState) -> AgentState:\n",
    "    try:\n",
    "        results = tavily_search.results(state[\"query\"], max_results=5)\n",
    "        snippets = \"\\n\\n\".join([r[\"content\"] for r in results])\n",
    "        answer = f\"Web search results:\\n{snippets}\"\n",
    "    except Exception as e:\n",
    "        answer = f\"Web search failed: {e}\"\n",
    "    return {\"query\": state[\"query\"], \"route\": \"web_search\", \"context\": answer}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdadd9e",
   "metadata": {},
   "source": [
    "## Define the RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e55c19af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF loading and vector store\n",
    "pdf_folder_path = \"./pdf_files\"\n",
    "persist_dir = \"./chroma_db\"\n",
    "\n",
    "def load_and_split_documents():\n",
    "    documents = []\n",
    "    for file_name in os.listdir(pdf_folder_path):\n",
    "        if file_name.lower().endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(os.path.join(pdf_folder_path, file_name))\n",
    "            docs = loader.load()\n",
    "            documents.extend(docs)\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=200)\n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "def setup_chroma_vectorstore():\n",
    "    if not os.path.exists(persist_dir) or not os.listdir(persist_dir):\n",
    "        print(\"üîß Initializing new Chroma DB...\")\n",
    "        splits = load_and_split_documents()\n",
    "        Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=embedding_model,\n",
    "            collection_name=\"rag-chroma\",\n",
    "            persist_directory=persist_dir\n",
    "        )\n",
    "    return Chroma(\n",
    "        persist_directory=persist_dir,\n",
    "        embedding_function=embedding_model,\n",
    "        collection_name=\"rag-chroma\"\n",
    "    )\n",
    "\n",
    "vectorstore = setup_chroma_vectorstore()\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "def rag_agent(state: AgentState) -> AgentState:\n",
    "    docs = retriever.get_relevant_documents(state[\"query\"])\n",
    "    context = \"\\n\\n---\\n\\n\".join(doc.page_content for doc in docs[:5])\n",
    "    return {\"query\": state[\"query\"], \"route\": \"rag\", \"context\": context}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b73c2d",
   "metadata": {},
   "source": [
    "## Define Routing Agent\n",
    "- For example: ‚ÄòLatest‚Äô, ‚Äòcurrent‚Äô in query leads to websearch \n",
    "otherwise RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "697ad6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_decision(state: AgentState) -> str:\n",
    "    query = state[\"query\"].lower()\n",
    "    if any(k in query for k in [\"latest\", \"current\", \"today\", \"202\", \"news\"]):\n",
    "        route = \"web_search\"\n",
    "    elif any(k in query for k in [\"transformer\", \"attention\", \"encoder\", \"decoder\"]):\n",
    "        route = \"rag\"\n",
    "    else:\n",
    "        route = \"llm\"\n",
    "    \n",
    "    print(f\"üìç Routing to node: {route.upper()} for query ‚Üí '{state['query']}'\")\n",
    "    return route\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47857ebe",
   "metadata": {},
   "source": [
    "## Define LLM agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "968e408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_agent(state: AgentState) -> AgentState:\n",
    "    messages = [HumanMessage(content=state[\"query\"])]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"query\": state[\"query\"], \"route\": \"llm\", \"context\": response.content.strip()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acf2ae2",
   "metadata": {},
   "source": [
    "## Define Summarisation agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "052bbcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarization_agent(state: AgentState) -> AgentState:\n",
    "    prompt = f\"\"\"Query: {state['query']}\n",
    "\n",
    "Context:\n",
    "{state['context']}\n",
    "\n",
    "Summarize the above in a clear, concise, factual response.\"\"\"\n",
    "    messages = [\n",
    "        HumanMessage(content=prompt),\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\n",
    "        \"query\": state[\"query\"],\n",
    "        \"route\": state[\"route\"],\n",
    "        \"context\": response.content.strip()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35feb460",
   "metadata": {},
   "source": [
    "## Build Langgraph pipeline - memory added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "72961c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1457acbd0>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LangGraph pipeline\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "builder.add_node(\"router\", lambda x: x) ## pass\n",
    "builder.add_node(\"rag\", rag_agent)\n",
    "builder.add_node(\"web_search\", web_search_agent)\n",
    "builder.add_node(\"llm\", llm_agent)\n",
    "builder.add_node(\"summarize\", summarization_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f92087",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.set_entry_point(\"router\")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"router\",\n",
    "    route_decision, # if return string is rag or web_search or llm --> route them to respective nodes\n",
    "    {\n",
    "        \"rag\": \"rag\",\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"llm\": \"llm\"\n",
    "    }\n",
    ")\n",
    "\n",
    "builder.add_edge(\"rag\", \"summarize\")\n",
    "builder.add_edge(\"web_search\", \"summarize\")\n",
    "builder.add_edge(\"llm\", \"summarize\")\n",
    "\n",
    "builder.set_finish_point(\"summarize\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory) ## memory checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8a7a7816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80e10e8",
   "metadata": {},
   "source": [
    "## Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a66b5cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_support_agent(agent, prompt, user_session_id, verbose=False):\n",
    "    events = agent.stream(\n",
    "        {\"query\": prompt}, # initial state of the agent\n",
    "        {\"configurable\": {\"thread_id\": user_session_id}},\n",
    "        stream_mode=\"values\",\n",
    "    )\n",
    "\n",
    "    print('Running Agent. Please wait...')\n",
    "    for event in events:\n",
    "        if verbose:\n",
    "                print(event)\n",
    "\n",
    "    display(Markdown(event['context']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f0d66",
   "metadata": {},
   "source": [
    "## Lets see examples of routing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe180d9",
   "metadata": {},
   "source": [
    "### when it uses RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "61a90f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Agent. Please wait...\n",
      "{'query': 'What are the advantages of multi-head attention in transformers?', 'context': 'The advantages of multi-head attention in transformers include:\\n\\n1. **Joint attention to different representation subspaces**: Multi-head attention allows the model to attend to information from different subspaces at different positions, which is not possible with a single attention head.\\n2. **Reduced dimensionality**: By reducing the dimensionality of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n3. **Improved ability to capture long-distance dependencies**: Multi-head attention enables the model to capture long-distance dependencies in the input sequence, as shown in Figure 3.\\n4. **Enhanced ability to resolve anaphora**: Multi-head attention also enables the model to resolve anaphora, as shown in Figure 4.\\n5. **Flexibility in attention mechanisms**: Multi-head attention allows for different attention mechanisms to be used in different layers, such as encoder-decoder attention, self-attention, and masked self-attention.\\n\\nOverall, multi-head attention is a key component of the transformer architecture that enables the model to capture complex relationships between input tokens and generate more accurate and informative outputs.', 'route': 'rag'}\n",
      "üìç Routing to node: RAG for query ‚Üí 'What are the advantages of multi-head attention in transformers?'\n",
      "{'query': 'What are the advantages of multi-head attention in transformers?', 'context': 'The advantages of multi-head attention in transformers include:\\n\\n1. **Joint attention to different representation subspaces**: Multi-head attention allows the model to attend to information from different subspaces at different positions, which is not possible with a single attention head.\\n2. **Reduced dimensionality**: By reducing the dimensionality of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n3. **Improved ability to capture long-distance dependencies**: Multi-head attention enables the model to capture long-distance dependencies in the input sequence, as shown in Figure 3.\\n4. **Enhanced ability to resolve anaphora**: Multi-head attention also enables the model to resolve anaphora, as shown in Figure 4.\\n5. **Flexibility in attention mechanisms**: Multi-head attention allows for different attention mechanisms to be used in different layers, such as encoder-decoder attention, self-attention, and masked self-attention.\\n\\nOverall, multi-head attention is a key component of the transformer architecture that enables the model to capture complex relationships between input tokens and generate more accurate and informative outputs.', 'route': 'rag'}\n",
      "{'query': 'What are the advantages of multi-head attention in transformers?', 'context': 'output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ‚àà Rdmodel√ódk , WK\\ni ‚àà Rdmodel√ódk , WV\\ni ‚àà Rdmodel√ódv\\nand WO ‚àà Rhdv√ódmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n‚Ä¢ In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n‚Ä¢ The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n‚Ä¢ Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to ‚àí‚àû) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\n\\n---\\n\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\n\\n---\\n\\nAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‚Äòmaking‚Äô, completing the phrase ‚Äòmaking...more difficult‚Äô. Attentions here shown only for\\nthe word ‚Äòmaking‚Äô. Different colors represent different heads. Best viewed in color.\\n13\\n\\n---\\n\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‚Äòits‚Äô for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', 'route': 'rag'}\n",
      "{'query': 'What are the advantages of multi-head attention in transformers?', 'context': 'The advantages of multi-head attention in transformers include:\\n\\n1. **Joint attention to different representation subspaces**: Multi-head attention allows the model to attend to information from different subspaces at different positions, which is not possible with a single attention head.\\n2. **Reduced dimensionality**: By reducing the dimensionality of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n3. **Improved ability to capture long-distance dependencies**: Multi-head attention enables the model to capture long-distance dependencies in the input sequence, as shown in Figure 3.\\n4. **Enhanced ability to resolve anaphora**: Multi-head attention also enables the model to resolve anaphora, as shown in Figure 4.\\n5. **Flexibility in attention mechanisms**: Multi-head attention allows for different attention mechanisms to be used in different layers, such as encoder-decoder attention, self-attention, and masked self-attention.\\n\\nOverall, multi-head attention is a key component of the transformer architecture that enables the model to capture complex relationships between input tokens and generate more accurate and informative outputs.', 'route': 'rag'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The advantages of multi-head attention in transformers include:\n",
       "\n",
       "1. **Joint attention to different representation subspaces**: Multi-head attention allows the model to attend to information from different subspaces at different positions, which is not possible with a single attention head.\n",
       "2. **Reduced dimensionality**: By reducing the dimensionality of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
       "3. **Improved ability to capture long-distance dependencies**: Multi-head attention enables the model to capture long-distance dependencies in the input sequence, as shown in Figure 3.\n",
       "4. **Enhanced ability to resolve anaphora**: Multi-head attention also enables the model to resolve anaphora, as shown in Figure 4.\n",
       "5. **Flexibility in attention mechanisms**: Multi-head attention allows for different attention mechanisms to be used in different layers, such as encoder-decoder attention, self-attention, and masked self-attention.\n",
       "\n",
       "Overall, multi-head attention is a key component of the transformer architecture that enables the model to capture complex relationships between input tokens and generate more accurate and informative outputs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "uid = 'subham001'\n",
    "query = \"What are the advantages of multi-head attention in transformers?\"\n",
    "call_support_agent(agent=graph,\n",
    "                   prompt=query,\n",
    "                   user_session_id=uid,\n",
    "                   verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d83295",
   "metadata": {},
   "source": [
    "### when it uses LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "69c30b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Agent. Please wait...\n",
      "{'query': 'Get the stock price details of nvidia.', 'context': 'The advantages of multi-head attention in transformers include:\\n\\n1. **Joint attention to different representation subspaces**: Multi-head attention allows the model to attend to information from different subspaces at different positions, which is not possible with a single attention head.\\n2. **Reduced dimensionality**: By reducing the dimensionality of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n3. **Improved ability to capture long-distance dependencies**: Multi-head attention enables the model to capture long-distance dependencies in the input sequence, as shown in Figure 3.\\n4. **Enhanced ability to resolve anaphora**: Multi-head attention also enables the model to resolve anaphora, as shown in Figure 4.\\n5. **Flexibility in attention mechanisms**: Multi-head attention allows for different attention mechanisms to be used in different layers, such as encoder-decoder attention, self-attention, and masked self-attention.\\n\\nOverall, multi-head attention is a key component of the transformer architecture that enables the model to capture complex relationships between input tokens and generate more accurate and informative outputs.', 'route': 'rag'}\n",
      "üìç Routing to node: LLM for query ‚Üí 'Get the stock price details of nvidia.'\n",
      "{'query': 'Get the stock price details of nvidia.', 'context': 'The advantages of multi-head attention in transformers include:\\n\\n1. **Joint attention to different representation subspaces**: Multi-head attention allows the model to attend to information from different subspaces at different positions, which is not possible with a single attention head.\\n2. **Reduced dimensionality**: By reducing the dimensionality of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n3. **Improved ability to capture long-distance dependencies**: Multi-head attention enables the model to capture long-distance dependencies in the input sequence, as shown in Figure 3.\\n4. **Enhanced ability to resolve anaphora**: Multi-head attention also enables the model to resolve anaphora, as shown in Figure 4.\\n5. **Flexibility in attention mechanisms**: Multi-head attention allows for different attention mechanisms to be used in different layers, such as encoder-decoder attention, self-attention, and masked self-attention.\\n\\nOverall, multi-head attention is a key component of the transformer architecture that enables the model to capture complex relationships between input tokens and generate more accurate and informative outputs.', 'route': 'rag'}\n",
      "{'query': 'Get the stock price details of nvidia.', 'context': 'I can provide you with the current stock price details of NVIDIA. However, please note that the stock prices may fluctuate rapidly and may not reflect the current market price.\\n\\nAs of my knowledge cutoff in December 2023, here are the stock price details of NVIDIA:\\n\\n**Company Information:**\\n- Company Name: NVIDIA Corporation\\n- Stock Symbol: NVDA\\n- Industry: Semiconductors\\n- Sector: Technology\\n\\n**Stock Price Details (as of December 2023):**\\n- Current Stock Price: $864.45 USD\\n- 52-Week High: $1,024.51 USD\\n- 52-Week Low: $644.55 USD\\n- Market Capitalization: $1.23 Trillion USD\\n- Dividend Yield: 0.23%\\n- P/E Ratio: 43.45\\n\\n**Historical Stock Price:**\\n- 1-Year Return: 44.45%\\n- 5-Year Return: 1,144.45%\\n- 10-Year Return: 2,444.45%\\n\\nPlease note that these figures are subject to change and may not reflect the current market price. For the most up-to-date information, I recommend checking a reliable financial website or platform, such as Yahoo Finance, Google Finance, or Bloomberg.\\n\\nTo get the current stock price, you can visit the following websites:\\n\\n- Yahoo Finance: [finance.yahoo.com](http://finance.yahoo.com)\\n- Google Finance: [finance.google.com](http://finance.google.com)\\n- Bloomberg: [www.bloomberg.com](http://www.bloomberg.com)\\n\\nPlease keep in mind that the stock market is subject to fluctuations, and the information provided may not reflect the current market conditions.', 'route': 'llm'}\n",
      "{'query': 'Get the stock price details of nvidia.', 'context': 'As of my knowledge cutoff in December 2023, the stock price details of NVIDIA Corporation (NVDA) are as follows:\\n\\n- Company Name: NVIDIA Corporation\\n- Stock Symbol: NVDA\\n- Industry: Semiconductors\\n- Sector: Technology\\n\\n**Stock Price Details:**\\n- Current Stock Price: $864.45 USD\\n- 52-Week High: $1,024.51 USD\\n- 52-Week Low: $644.55 USD\\n- Market Capitalization: $1.23 Trillion USD\\n- Dividend Yield: 0.23%\\n- P/E Ratio: 43.45\\n\\n**Historical Stock Price:**\\n- 1-Year Return: 44.45%\\n- 5-Year Return: 1,144.45%\\n- 10-Year Return: 2,444.45%\\n\\nPlease note that these figures are subject to change and may not reflect the current market price. For the most up-to-date information, I recommend checking a reliable financial website or platform.', 'route': 'llm'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "As of my knowledge cutoff in December 2023, the stock price details of NVIDIA Corporation (NVDA) are as follows:\n",
       "\n",
       "- Company Name: NVIDIA Corporation\n",
       "- Stock Symbol: NVDA\n",
       "- Industry: Semiconductors\n",
       "- Sector: Technology\n",
       "\n",
       "**Stock Price Details:**\n",
       "- Current Stock Price: $864.45 USD\n",
       "- 52-Week High: $1,024.51 USD\n",
       "- 52-Week Low: $644.55 USD\n",
       "- Market Capitalization: $1.23 Trillion USD\n",
       "- Dividend Yield: 0.23%\n",
       "- P/E Ratio: 43.45\n",
       "\n",
       "**Historical Stock Price:**\n",
       "- 1-Year Return: 44.45%\n",
       "- 5-Year Return: 1,144.45%\n",
       "- 10-Year Return: 2,444.45%\n",
       "\n",
       "Please note that these figures are subject to change and may not reflect the current market price. For the most up-to-date information, I recommend checking a reliable financial website or platform."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "uid = 'subham001'\n",
    "query = \"Get the stock price details of nvidia.\"\n",
    "call_support_agent(agent=graph,\n",
    "                   prompt=query,\n",
    "                   user_session_id=uid,\n",
    "                   verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673575eb",
   "metadata": {},
   "source": [
    "### when it uses Web search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "200454f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Agent. Please wait...\n",
      "{'query': 'Get the stock price details of nvidia in 2025', 'context': 'As of my knowledge cutoff in December 2023, the stock price details of NVIDIA Corporation (NVDA) are as follows:\\n\\n- Company Name: NVIDIA Corporation\\n- Stock Symbol: NVDA\\n- Industry: Semiconductors\\n- Sector: Technology\\n\\n**Stock Price Details:**\\n- Current Stock Price: $864.45 USD\\n- 52-Week High: $1,024.51 USD\\n- 52-Week Low: $644.55 USD\\n- Market Capitalization: $1.23 Trillion USD\\n- Dividend Yield: 0.23%\\n- P/E Ratio: 43.45\\n\\n**Historical Stock Price:**\\n- 1-Year Return: 44.45%\\n- 5-Year Return: 1,144.45%\\n- 10-Year Return: 2,444.45%\\n\\nPlease note that these figures are subject to change and may not reflect the current market price. For the most up-to-date information, I recommend checking a reliable financial website or platform.', 'route': 'llm'}\n",
      "üìç Routing to node: WEB_SEARCH for query ‚Üí 'Get the stock price details of nvidia in 2025'\n",
      "{'query': 'Get the stock price details of nvidia in 2025', 'context': 'As of my knowledge cutoff in December 2023, the stock price details of NVIDIA Corporation (NVDA) are as follows:\\n\\n- Company Name: NVIDIA Corporation\\n- Stock Symbol: NVDA\\n- Industry: Semiconductors\\n- Sector: Technology\\n\\n**Stock Price Details:**\\n- Current Stock Price: $864.45 USD\\n- 52-Week High: $1,024.51 USD\\n- 52-Week Low: $644.55 USD\\n- Market Capitalization: $1.23 Trillion USD\\n- Dividend Yield: 0.23%\\n- P/E Ratio: 43.45\\n\\n**Historical Stock Price:**\\n- 1-Year Return: 44.45%\\n- 5-Year Return: 1,144.45%\\n- 10-Year Return: 2,444.45%\\n\\nPlease note that these figures are subject to change and may not reflect the current market price. For the most up-to-date information, I recommend checking a reliable financial website or platform.', 'route': 'llm'}\n",
      "{'query': 'Get the stock price details of nvidia in 2025', 'context': 'Web search results:\\nXNAS:NVDA,NASD:NVDA historical stock data| Stock Date | Stock Price |\\n| --- | --- |\\n| July 01, 2025 | $153.3 |\\n| June 30, 2025 | $157.99 |\\n| June 27, 2025 | $157.75 |\\n| June 26, 2025 | $155.02 |\\n| June 25, 2025 | $154.31 |\\n| June 24, 2025 | $147.9 |\\n| June 23, 2025 | $144.17 |\\n| June 20, 2025 | $143.85 |\\n| June 18, 2025 | $145.48 |\\n| June 17, 2025 | $144.12 |\\n| June 16, 2025 | $144.69 |\\n| June 13, 2025 | $141.97 |\\n| June 12, 2025 | $145 |\\n| June 11, 2025 | $142.83 |\\n| June 10, 2025 | $143.96 | [...] | June 09, 2025 | $142.63 |\\n| June 06, 2025 | $141.72 |\\n| June 05, 2025 | $139.99 |\\n| June 04, 2025 | $141.92 |\\n| June 03, 2025 | $141.22 |\\n| June 02, 2025 | $137.38 |\\n| May 30, 2025 | $135.13 |\\n| May 29, 2025 | $139.19 |\\n| May 28, 2025 | $134.81 |\\n| May 27, 2025 | $135.5 |\\n| May 23, 2025 | $131.29 |\\n| May 22, 2025 | $132.83 |\\n| May 21, 2025 | $131.8 |\\n| May 20, 2025 | $134.38 |\\n| May 19, 2025 | $135.57 |\\n| May 16, 2025 | $135.4 |\\n| May 15, 2025 | $134.83 |\\n| May 14, 2025 | $135.34 | [...] | May 13, 2025 | $129.93 |\\n| May 12, 2025 | $123 |\\n| May 09, 2025 | $116.65 |\\n| May 08, 2025 | $117.37 |\\n| May 07, 2025 | $117.06 |\\n| May 06, 2025 | $113.54 |\\n| May 05, 2025 | $113.82 |\\n| May 02, 2025 | $114.5 |\\n| May 01, 2025 | $111.61 |\\n| April 30, 2025 | $108.92 |\\n| April 29, 2025 | $109.02 |\\n| April 28, 2025 | $108.73 |\\n| April 25, 2025 | $111.01 |\\n| April 24, 2025 | $106.43 |\\n| April 23, 2025 | $102.71 |\\n| April 22, 2025 | $98.89 |\\n| April 21, 2025 | $96.91 |\\n| April 17, 2025 | $101.49 |\\n\\n| Apr 2, 2025 | 107.29 | 111.98 | 106.79 | 110.42 | 110.41 | 220,601,200 |\\n| Apr 1, 2025 | 108.52 | 110.20 | 106.47 | 110.15 | 110.14 | 222,614,000 |\\n| Mar 31, 2025 | 105.13 | 110.96 | 103.65 | 108.38 | 108.37 | 299,212,700 |\\n| Mar 28, 2025 | 111.49 | 112.87 | 109.07 | 109.67 | 109.66 | 229,872,500 |\\n| Mar 27, 2025 | 111.35 | 114.45 | 110.66 | 111.43 | 111.42 | 236,902,100 |\\n| Mar 26, 2025 | 118.73 | 118.84 | 112.71 | 113.76 | 113.75 | 293,463,300 | [...] | Mar 17, 2025 | 122.74 | 122.89 | 118.03 | 119.53 | 119.52 | 255,501,500 |\\n| Mar 14, 2025 | 118.61 | 121.88 | 118.15 | 121.67 | 121.66 | 277,593,500 |\\n| Mar 13, 2025 | 117.03 | 117.76 | 113.79 | 115.58 | 115.57 | 299,033,100 |\\n| Mar 12, 2025 | 0.01 Dividend |\\n| Mar 12, 2025 | 114.12 | 116.76 | 112.88 | 115.74 | 115.73 | 323,857,500 |\\n| Mar 11, 2025 | 106.99 | 112.24 | 104.77 | 108.76 | 108.74 | 354,865,700 |\\n| Mar 10, 2025 | 109.90 | 111.85 | 105.46 | 106.98 | 106.96 | 366,487,400 | [...] | Date | Open | High | Low | Close Close price adjusted for splits. | Adj Close Adjusted close price adjusted for splits and dividend and/or capital gain distributions. | Volume |\\n| --- | --- | --- | --- | --- | --- | --- |\\n| Jul 3, 2025 | 158.37 | 160.98 | 157.77 | 159.34 | 159.34 | 143,716,100 |\\n| Jul 2, 2025 | 152.98 | 157.60 | 152.97 | 157.25 | 157.25 | 171,224,100 |\\n| Jul 1, 2025 | 156.29 | 157.20 | 151.49 | 153.30 | 153.30 | 213,143,600 |\\n\\n### How much has Nvidia stock rallied in 2025 so far?\\n\\nNvidia has staged a stunning comeback this year, gaining 39% year-to-date and recapturing its $1.4 trillion market cap milestone. The rally is fueled by:  \\n  \\n\\n Massive demand for AI infrastructure\\n Robust sales of the next-gen Blackwell AI chips\\n Optimism about easing U.S.‚ÄìChina export tensions\\n\\nAs of July 2, 2025, NVDA is trading at $153.30, down slightly by -0.03% intraday, but still hovering near its all-time high.\\n\\nAccording to our NVIDIA stock prediction for 2025, NVDA stock will be priced between $ 127.18 and $ 152.57 in 2025.\\n\\n### What is the NVIDIA stock prediction for 2030?\\n\\nAccording to our NVIDIA stock prediction for 2030, NVDA stock will be priced between $ 262.70 and $ 356.91 in 2030\\n\\n### Will NVIDIA stock reach $500?\\n\\nNVIDIA stock would need to gain 238.07% to reach $500. According to our NVIDIA stock forecast algorithm, NVIDIA stock will hit $500 in May 6, 2032.', 'route': 'web_search'}\n",
      "{'query': 'Get the stock price details of nvidia in 2025', 'context': 'Based on the provided web search results, here are the key points about Nvidia stock in 2025:\\n\\n1. As of July 1, 2025, the stock price of Nvidia (NVDA) is $153.30.\\n2. The stock has rallied 39% year-to-date in 2025.\\n3. The rally is fueled by massive demand for AI infrastructure, robust sales of the next-gen Blackwell AI chips, and optimism about easing U.S.‚ÄìChina export tensions.\\n4. According to the provided stock prediction, the stock price of NVDA is expected to be between $127.18 and $152.57 in 2025.\\n5. The stock price of NVDA is expected to reach $500 in May 6, 2032, according to the provided stock forecast algorithm.\\n\\nNote that the provided stock predictions and forecasts are based on the information available at the time of the search and may not reflect the actual stock performance.', 'route': 'web_search'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Based on the provided web search results, here are the key points about Nvidia stock in 2025:\n",
       "\n",
       "1. As of July 1, 2025, the stock price of Nvidia (NVDA) is $153.30.\n",
       "2. The stock has rallied 39% year-to-date in 2025.\n",
       "3. The rally is fueled by massive demand for AI infrastructure, robust sales of the next-gen Blackwell AI chips, and optimism about easing U.S.‚ÄìChina export tensions.\n",
       "4. According to the provided stock prediction, the stock price of NVDA is expected to be between $127.18 and $152.57 in 2025.\n",
       "5. The stock price of NVDA is expected to reach $500 in May 6, 2032, according to the provided stock forecast algorithm.\n",
       "\n",
       "Note that the provided stock predictions and forecasts are based on the information available at the time of the search and may not reflect the actual stock performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "uid = 'subham001'\n",
    "query = \"Get the stock price details of nvidia in 2025\"\n",
    "call_support_agent(agent=graph,\n",
    "                   prompt=query,\n",
    "                   user_session_id=uid,\n",
    "                   verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db070b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
